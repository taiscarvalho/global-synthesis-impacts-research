{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fulltext = pd.read_csv(\"data/papers_to_annotate_fulltext.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenized_sentences(sentences):\n",
    "    new_sentences = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        current_sentence = sentences[i]\n",
    "        next_sentence = sentences[i + 1] if i + 1 < len(sentences) else \"\"\n",
    "        combined_sentence = current_sentence + \" \" + next_sentence\n",
    "\n",
    "        # Define a list of sentence endings to check\n",
    "        endings_to_check = [\"et al\\.$\", \"cf\\.$\", \"eg\\.$\", \"Fig\\.$\", \"in\\.$\", \"m\\.$\", r\"\\d{4}\\;$\"]\n",
    "\n",
    "        # Check if the current sentence ends with any of the specified endings\n",
    "        for ending_pattern in endings_to_check:\n",
    "            if re.search(ending_pattern, current_sentence):\n",
    "                new_sentences.append(combined_sentence)\n",
    "                i += 2  # Skip the next sentence since it's been joined\n",
    "                break\n",
    "        else:\n",
    "            # If none of the specified endings are found, add the current sentence as is\n",
    "            new_sentences.append(current_sentence)\n",
    "            i += 1\n",
    "\n",
    "    # Filter out sentences with less than 3 words or with only single-character words\n",
    "    new_sentences = [sentence for sentence in new_sentences if len(word_tokenize(sentence)) > 3]\n",
    "    new_sentences = [sentence for sentence in new_sentences if not all(len(word) == 1 for word in word_tokenize(sentence))]\n",
    "\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sentences(sentences):\n",
    "    merged_sentences = []\n",
    "    current_sentence = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if '(' in sentence and ')' not in sentence:\n",
    "            # Start merging sentences when an opening parenthesis is found without a closing parenthesis\n",
    "            current_sentence = sentence\n",
    "        elif ')' in sentence and current_sentence:\n",
    "            # If a closing parenthesis is found, join it with the current sentence\n",
    "            current_sentence += \" \" + sentence\n",
    "            merged_sentences.append(current_sentence)\n",
    "            current_sentence = \"\"\n",
    "        elif current_sentence:\n",
    "            # Continue merging if the current sentence does not contain a closing parenthesis\n",
    "            current_sentence += \" \" + sentence\n",
    "        else:\n",
    "            # If there's no current sentence, add the sentence as it is\n",
    "            merged_sentences.append(sentence)\n",
    "\n",
    "    # Join the merged sentences to form the final result\n",
    "    #result = ' '.join(merged_sentences)\n",
    "\n",
    "    return merged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizing(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    sentences = clean_tokenized_sentences(sentences)\n",
    "    sentences = merge_sentences(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text(text):\n",
    "    end_words = [\"Funding\", \"Author statement\", \"CRediT authorship\"]\n",
    "    last_occurrence = -1\n",
    "    for word in end_words:\n",
    "        \n",
    "        index = text.rfind(word)\n",
    "        if index > last_occurrence:\n",
    "            last_occurrence = index\n",
    "\n",
    "        if last_occurrence >= 0:\n",
    "            # Remove all text that comes after the last occurrence\n",
    "            text = text[:last_occurrence]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fulltext[\"text_cut\"] = df_fulltext[\"full_text\"].apply(lambda x: remove_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df_fulltext[\"text_cut\"].apply(lambda x: sentence_tokenizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for text in df_sentences:\n",
    "    title = ''.join(e for e in df_fulltext[\"title\"][i] if e.isalnum())\n",
    "    file_name = \"data/tokenized/tokenized_text_\" + str(title) + \".txt\"\n",
    "    with open(file_name, \"w\") as f:\n",
    "        for line in text:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
